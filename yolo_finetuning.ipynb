{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# YOLO Fine-tuning for Bird Detection\n",
        "\n",
        "This notebook demonstrates how to fine-tune a YOLOv8 model using your own bird pictures for improved detection performance on your specific use case.\n",
        "\n",
        "## Overview\n",
        "1. **Environment Setup** - Install dependencies and import libraries\n",
        "2. **Data Preparation** - Organize your images and create annotations\n",
        "3. **Dataset Configuration** - Set up YOLO dataset format\n",
        "4. **Model Loading** - Load the pre-trained YOLOv8 model\n",
        "5. **Training Configuration** - Set hyperparameters and training options\n",
        "6. **Fine-tuning** - Train the model on your data\n",
        "7. **Evaluation** - Test the fine-tuned model\n",
        "8. **Export** - Save the trained model for deployment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ultralytics in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (8.3.153)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from ultralytics) (3.10.3)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from ultralytics) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from ultralytics) (11.2.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from ultralytics) (1.15.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from ultralytics) (0.17.2)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from ultralytics) (7.0.0)\n",
            "Requirement already satisfied: py-cpuinfo in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from ultralytics) (2.3.0)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from ultralytics) (2.0.14)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (4.58.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (2025.4.26)\n",
            "Requirement already satisfied: filelock in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (4.14.0)\n",
            "Requirement already satisfied: sympy in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
            "Requirement already satisfied: networkx in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (2025.5.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: pillow in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (11.2.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: matplotlib in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (3.10.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from matplotlib) (4.58.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: opencv-python in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from opencv-python) (1.26.4)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: pyyaml in /Users/kevincorstorphine/.pyenv/versions/3.11.6/lib/python3.11/site-packages (6.0.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "%pip install ultralytics\n",
        "%pip install pillow\n",
        "%pip install matplotlib\n",
        "%pip install opencv-python\n",
        "%pip install pyyaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NumPy version: 1.26.4\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "print(f\"NumPy version: {np.__version__}\")  # Should show 1.26.4 or similar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setup complete! Dataset: /Users/kevincorstorphine/Desktop/Bird_Soap/bird_dataset\n"
          ]
        }
      ],
      "source": [
        "import os, cv2, yaml, shutil\n",
        "from pathlib import Path\n",
        "from ultralytics import YOLO\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Setup directories\n",
        "root = Path.cwd()\n",
        "dirs = {\n",
        "    'train_img': root / \"bird_dataset/images/train\",\n",
        "    'val_img': root / \"bird_dataset/images/val\", \n",
        "    'train_lbl': root / \"bird_dataset/labels/train\",\n",
        "    'val_lbl': root / \"bird_dataset/labels/val\"\n",
        "}\n",
        "for d in dirs.values():\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Setup complete! Dataset: {root / 'bird_dataset'}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Copy & Convert Images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copied 10 images\n"
          ]
        }
      ],
      "source": [
        "source_path = \"/Users/kevincorstorphine/Desktop/Bird_Soap/finetunepics\"\n",
        "\n",
        "def copy_images(src, dst):\n",
        "    count = 0\n",
        "    for f in os.listdir(src):\n",
        "        if not os.path.isfile(os.path.join(src, f)):\n",
        "            continue\n",
        "        ext = f.lower().split('.')[-1]\n",
        "        if ext in ['jpg', 'jpeg', 'png', 'bmp']:\n",
        "            shutil.copy2(os.path.join(src, f), dst)\n",
        "            count += 1\n",
        "        elif ext in ['heic', 'heif']:\n",
        "            try:\n",
        "                Image.open(os.path.join(src, f)).save(\n",
        "                    os.path.join(dst, f.rsplit('.', 1)[0] + '.jpg'), 'JPEG')\n",
        "                count += 1\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to convert {f}: {e}\")\n",
        "    return count\n",
        "\n",
        "copied = copy_images(source_path, dirs['train_img'])\n",
        "print(f\"Copied {copied} images\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Generate Annotations & Split Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "image 1/1 /Users/kevincorstorphine/Desktop/Bird_Soap/bird_dataset/images/train/IMG_1355.jpg: 640x480 1 traffic light, 1 bird, 82.5ms\n",
            "Speed: 3.1ms preprocess, 82.5ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /Users/kevincorstorphine/Desktop/Bird_Soap/bird_dataset/images/train/IMG_1354.jpg: 640x480 (no detections), 95.1ms\n",
            "Speed: 2.2ms preprocess, 95.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /Users/kevincorstorphine/Desktop/Bird_Soap/bird_dataset/images/train/IMG_1356.jpg: 640x480 2 birds, 218.8ms\n",
            "Speed: 2.3ms preprocess, 218.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /Users/kevincorstorphine/Desktop/Bird_Soap/bird_dataset/images/train/IMG_1357.jpg: 640x480 (no detections), 214.9ms\n",
            "Speed: 12.2ms preprocess, 214.9ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /Users/kevincorstorphine/Desktop/Bird_Soap/bird_dataset/images/train/IMG_1353.jpg: 640x480 1 bird, 121.3ms\n",
            "Speed: 2.4ms preprocess, 121.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /Users/kevincorstorphine/Desktop/Bird_Soap/bird_dataset/images/train/IMG_1352.jpg: 640x480 1 bird, 82.9ms\n",
            "Speed: 4.6ms preprocess, 82.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /Users/kevincorstorphine/Desktop/Bird_Soap/bird_dataset/images/train/IMG_1351.jpg: 640x480 1 bird, 61.0ms\n",
            "Speed: 2.5ms preprocess, 61.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /Users/kevincorstorphine/Desktop/Bird_Soap/bird_dataset/images/train/IMG_1360.jpg: 640x480 2 birds, 59.0ms\n",
            "Speed: 2.2ms preprocess, 59.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /Users/kevincorstorphine/Desktop/Bird_Soap/bird_dataset/images/train/IMG_1359.jpg: 640x480 1 bird, 62.3ms\n",
            "Speed: 2.1ms preprocess, 62.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /Users/kevincorstorphine/Desktop/Bird_Soap/bird_dataset/images/train/IMG_1358.jpg: 640x480 1 bird, 84.0ms\n",
            "Speed: 2.5ms preprocess, 84.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Created 6 annotations\n",
            "Split: 8 train, 2 validation\n"
          ]
        }
      ],
      "source": [
        "model = YOLO('yolov8n.pt')\n",
        "\n",
        "def annotate_images(img_dir, lbl_dir):\n",
        "    count = 0\n",
        "    for img_file in os.listdir(img_dir):\n",
        "        if not img_file.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n",
        "            continue\n",
        "            \n",
        "        img_path = os.path.join(img_dir, img_file)\n",
        "        results = model(img_path)\n",
        "        \n",
        "        annotations = []\n",
        "        if results[0].boxes is not None:\n",
        "            img = cv2.imread(img_path)\n",
        "            h, w = img.shape[:2]\n",
        "            \n",
        "            for box in results[0].boxes:\n",
        "                if box.cls[0] == 14 and box.conf[0] > 0.5:  # Bird class\n",
        "                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
        "                    # Convert to YOLO format\n",
        "                    cx, cy = (x1+x2)/(2*w), (y1+y2)/(2*h)\n",
        "                    bw, bh = (x2-x1)/w, (y2-y1)/h\n",
        "                    annotations.append(f\"0 {cx:.6f} {cy:.6f} {bw:.6f} {bh:.6f}\")\n",
        "        \n",
        "        if annotations:\n",
        "            lbl_path = os.path.join(lbl_dir, img_file.rsplit('.', 1)[0] + '.txt')\n",
        "            with open(lbl_path, 'w') as f:\n",
        "                f.write('\\n'.join(annotations))\n",
        "            count += 1\n",
        "    return count\n",
        "\n",
        "# Generate annotations and split dataset\n",
        "annotated = annotate_images(dirs['train_img'], dirs['train_lbl'])\n",
        "print(f\"Created {annotated} annotations\")\n",
        "\n",
        "# Split to validation (20%)\n",
        "img_files = [f for f in os.listdir(dirs['train_img']) if f.endswith('.jpg')]\n",
        "val_files = np.random.choice(img_files, int(len(img_files)*0.2), replace=False)\n",
        "\n",
        "for f in val_files:\n",
        "    shutil.move(dirs['train_img'] / f, dirs['val_img'] / f)\n",
        "    lbl_f = f.rsplit('.', 1)[0] + '.txt'\n",
        "    if (dirs['train_lbl'] / lbl_f).exists():\n",
        "        shutil.move(dirs['train_lbl'] / lbl_f, dirs['val_lbl'] / lbl_f)\n",
        "\n",
        "print(f\"Split: {len(img_files) - len(val_files)} train, {len(val_files)} validation\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Train Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.153 ðŸš€ Python-3.11.6 torch-2.2.2 CPU (Intel Core(TM) i9-9980HK 2.40GHz)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/Users/kevincorstorphine/Desktop/Bird_Soap/bird_dataset/dataset.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=5, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=bird_model, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/Users/kevincorstorphine/Desktop/Bird_Soap, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/Users/kevincorstorphine/Desktop/Bird_Soap/bird_model, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
            "Model summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 2265.9Â±535.8 MB/s, size: 3769.9 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/kevincorstorphine/Desktop/Bird_Soap/bird_dataset/labels/train... 5 images, 3 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 3608.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/kevincorstorphine/Desktop/Bird_Soap/bird_dataset/labels/train.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 2584.9Â±1459.3 MB/s, size: 3916.3 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/kevincorstorphine/Desktop/Bird_Soap/bird_dataset/labels/val... 2 images, 1 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 1250.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Users/kevincorstorphine/Desktop/Bird_Soap/bird_dataset/labels/val.cache\n",
            "Plotting labels to /Users/kevincorstorphine/Desktop/Bird_Soap/bird_model/labels.jpg... \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1m/Users/kevincorstorphine/Desktop/Bird_Soap/bird_model\u001b[0m\n",
            "Starting training for 5 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "        1/5         0G      1.532      4.027      1.633          9        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.08s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.12it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          3          2    0.00222          1      0.497      0.497\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "        2/5         0G     0.9786      3.671      1.268         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.46s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          3          2    0.00222          1      0.497      0.497\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "        3/5         0G      1.229      4.115      1.296         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.32s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          3          2    0.00222          1      0.497      0.497\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "        4/5         0G      1.103      3.817      1.257         16        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.85s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.95it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          3          2    0.00222          1      0.497      0.497\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "        5/5         0G     0.7024      3.701     0.9894         12        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.87s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          3          2    0.00222          1      0.497      0.497\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "5 epochs completed in 0.005 hours.\n",
            "Optimizer stripped from /Users/kevincorstorphine/Desktop/Bird_Soap/bird_model/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from /Users/kevincorstorphine/Desktop/Bird_Soap/bird_model/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating /Users/kevincorstorphine/Desktop/Bird_Soap/bird_model/weights/best.pt...\n",
            "Ultralytics 8.3.153 ðŸš€ Python-3.11.6 torch-2.2.2 CPU (Intel Core(TM) i9-9980HK 2.40GHz)\n",
            "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all          3          2    0.00222          1      0.497      0.497\n",
            "Speed: 1.4ms preprocess, 73.3ms inference, 0.0ms loss, 2.9ms postprocess per image\n",
            "Results saved to \u001b[1m/Users/kevincorstorphine/Desktop/Bird_Soap/bird_model\u001b[0m\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "# Create dataset config\n",
        "config = {\n",
        "    'path': str((root / \"bird_dataset\").absolute()),\n",
        "    'train': 'images/train',\n",
        "    'val': 'images/val',\n",
        "    'nc': 1,\n",
        "    'names': ['bird']\n",
        "}\n",
        "\n",
        "config_path = root / \"bird_dataset/dataset.yaml\"\n",
        "with open(config_path, 'w') as f:\n",
        "    yaml.dump(config, f)\n",
        "\n",
        "# Train model\n",
        "model = YOLO('yolov8n.pt')\n",
        "results = model.train(\n",
        "    data=str(config_path),\n",
        "    epochs=5,\n",
        "    imgsz=640,\n",
        "    batch=16,\n",
        "    device='cpu',  # Change to 'cuda' if you have GPU\n",
        "    project=str(root),\n",
        "    name='bird_model',\n",
        "    exist_ok=True\n",
        ")\n",
        "\n",
        "print(\"Training complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "image 1/1 /Users/kevincorstorphine/Desktop/Bird_Soap/bird_dataset/images/val/IMG_1355.jpg: 640x480 (no detections), 96.3ms\n",
            "Speed: 2.2ms preprocess, 96.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Test on IMG_1355.jpg:\n",
            "\\nModel exported to: /Users/kevincorstorphine/Desktop/Bird_Soap/finetuned_bird_model.pt\n",
            "Use with: model = YOLO('/Users/kevincorstorphine/Desktop/Bird_Soap/finetuned_bird_model.pt')\n"
          ]
        }
      ],
      "source": [
        "# Load and test the trained model\n",
        "model_path = root / 'bird_model/weights/best.pt'\n",
        "if not model_path.exists():\n",
        "    model_path = root / 'bird_model/weights/last.pt'\n",
        "\n",
        "if model_path.exists():\n",
        "    trained_model = YOLO(str(model_path))\n",
        "    \n",
        "    # Test on a sample image\n",
        "    test_imgs = list(dirs['val_img'].glob('*.jpg')) or list(dirs['train_img'].glob('*.jpg'))\n",
        "    if test_imgs:\n",
        "        results = trained_model(str(test_imgs[0]))\n",
        "        print(f\"Test on {test_imgs[0].name}:\")\n",
        "        if results[0].boxes is not None:\n",
        "            for box in results[0].boxes:\n",
        "                if box.conf[0] > 0.5:\n",
        "                    print(f\"  Bird detected: {box.conf[0]:.3f} confidence\")\n",
        "        else:\n",
        "            print(\"  No birds detected\")\n",
        "    \n",
        "    # Export model\n",
        "    export_path = root / \"finetuned_bird_model.pt\"\n",
        "    shutil.copy2(model_path, export_path)\n",
        "    print(f\"\\\\nModel exported to: {export_path}\")\n",
        "    print(f\"Use with: model = YOLO('{export_path}')\")\n",
        "else:\n",
        "    print(\"No trained model found!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Optional: Visualize Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "image 1/1 /Users/kevincorstorphine/Desktop/Bird_Soap/bird_dataset/images/val/IMG_1355.jpg: 640x480 1 traffic light, 1 bird, 122.9ms\n",
            "Speed: 3.3ms preprocess, 122.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /Users/kevincorstorphine/Desktop/Bird_Soap/bird_dataset/images/val/IMG_1355.jpg: 640x480 (no detections), 80.9ms\n",
            "Speed: 3.3ms preprocess, 80.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /Users/kevincorstorphine/Desktop/Bird_Soap/bird_dataset/images/val/IMG_1353.jpg: 640x480 1 bird, 114.1ms\n",
            "Speed: 3.9ms preprocess, 114.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /Users/kevincorstorphine/Desktop/Bird_Soap/bird_dataset/images/val/IMG_1353.jpg: 640x480 (no detections), 88.2ms\n",
            "Speed: 2.5ms preprocess, 88.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Compare original vs fine-tuned model\n",
        "if model_path.exists():\n",
        "    original = YOLO('yolov8n.pt')\n",
        "    trained = YOLO(str(model_path))\n",
        "    \n",
        "    test_imgs = list(dirs['val_img'].glob('*.jpg'))[:2]\n",
        "    if not test_imgs:\n",
        "        test_imgs = list(dirs['train_img'].glob('*.jpg'))[:2]\n",
        "    \n",
        "    fig, axes = plt.subplots(2, len(test_imgs), figsize=(12, 8))\n",
        "    if len(test_imgs) == 1:\n",
        "        axes = axes.reshape(2, 1)\n",
        "    \n",
        "    for i, img_path in enumerate(test_imgs):\n",
        "        img = cv2.imread(str(img_path))\n",
        "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        # Original model\n",
        "        orig_results = original(str(img_path))\n",
        "        orig_img = img_rgb.copy()\n",
        "        if orig_results[0].boxes is not None:\n",
        "            for box in orig_results[0].boxes:\n",
        "                if box.cls[0] == 14 and box.conf[0] > 0.5:\n",
        "                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)\n",
        "                    cv2.rectangle(orig_img, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "        \n",
        "        # Fine-tuned model  \n",
        "        ft_results = trained(str(img_path))\n",
        "        ft_img = img_rgb.copy()\n",
        "        if ft_results[0].boxes is not None:\n",
        "            for box in ft_results[0].boxes:\n",
        "                if box.conf[0] > 0.5:\n",
        "                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)\n",
        "                    cv2.rectangle(ft_img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "        \n",
        "        axes[0, i].imshow(orig_img)\n",
        "        axes[0, i].set_title('Original Model')\n",
        "        axes[0, i].axis('off')\n",
        "        \n",
        "        axes[1, i].imshow(ft_img)\n",
        "        axes[1, i].set_title('Fine-tuned Model')  \n",
        "        axes[1, i].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
